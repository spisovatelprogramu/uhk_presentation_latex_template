
@book{group_united_nations_maternal_mortality_estimation_inter-agency_trends_2023,
	title = {Trends in maternal mortality 2000 to 2020: estimates by {WHO}, {UNICEF}, {UNFPA}, {World} {Bank} {Group} and {UNDESA}/{Population} {Division}},
	publisher = {World Health Organization},
	author = {Group United Nations Maternal Mortality Estimation Inter-agency},
	year = {2023},
	note = {Type: Publications},
}

@book{world_health_organization_who_2021,
	edition = {6th ed},
	title = {{WHO} laboratory manual for the examination and processing of human semen},
	publisher = {World Health Organization},
	author = {World Health Organization},
	year = {2021},
	note = {Type: Publications},
}

@incollection{rojas_sperm-cell_2022,
	address = {Cham},
	title = {Sperm-cell {Detection} {Using} {YOLOv5} {Architecture}},
	volume = {13347},
	url = {https://link.springer.com/10.1007/978-3-031-07802-6_27},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {Bioinformatics and {Biomedical} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Dobrovolny, Michal and Benes, Jakub and Krejcar, Ondrej and Selamat, Ali},
	editor = {Rojas, Ignacio and Valenzuela, Olga and Rojas, Fernando and Herrera, Luis Javier and Ortuño, Francisco},
	year = {2022},
	doi = {10.1007/978-3-031-07802-6_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {319--330},
}

@misc{deininger_comparative_2022,
	title = {A comparative study between vision transformers and {CNNs} in digital pathology},
	url = {http://arxiv.org/abs/2206.00389},
	doi = {10.48550/arXiv.2206.00389},
	abstract = {Recently, vision transformers were shown to be capable of outperforming convolutional neural networks when pretrained on sufficient amounts of data. In comparison to convolutional neural networks, vision transformers have a weaker inductive bias and therefore allow a more flexible feature detection. Due to their promising feature detection, this work explores vision transformers for tumor detection in digital pathology whole slide images in four tissue types, and for tissue type identification. We compared the patch-wise classification performance of the vision transformer DeiT-Tiny to the state-of-the-art convolutional neural network ResNet18. Due to the sparse availability of annotated whole slide images, we further compared both models pretrained on large amounts of unlabeled whole-slide images using state-of-the-art self-supervised approaches. The results show that the vision transformer performed slightly better than the ResNet18 for three of four tissue types for tumor detection while the ResNet18 performed slightly better for the remaining tasks. The aggregated predictions of both models on slide level were correlated, indicating that the models captured similar imaging features. All together, the vision transformer models performed on par with the ResNet18 while requiring more effort to train. In order to surpass the performance of convolutional neural networks, vision transformers might require more challenging tasks to benefit from their weak inductive bias.},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Deininger, Luca and Stimpel, Bernhard and Yuce, Anil and Abbasi-Sureshjani, Samaneh and Schönenberger, Simon and Ocampo, Paolo and Korski, Konstanty and Gaire, Fabien},
	month = jun,
	year = {2022},
	note = {arXiv:2206.00389},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{takahashi_comparison_2024,
	title = {Comparison of {Vision} {Transformers} and {Convolutional} {Neural} {Networks} in {Medical} {Image} {Analysis}: {A} {Systematic} {Review}},
	volume = {48},
	issn = {1573-689X},
	shorttitle = {Comparison of {Vision} {Transformers} and {Convolutional} {Neural} {Networks} in {Medical} {Image} {Analysis}},
	url = {https://doi.org/10.1007/s10916-024-02105-8},
	doi = {10.1007/s10916-024-02105-8},
	abstract = {In the rapidly evolving field of medical image analysis utilizing artificial intelligence (AI), the selection of appropriate computational models is critical for accurate diagnosis and patient care. This literature review provides a comprehensive comparison of vision transformers (ViTs) and convolutional neural networks (CNNs), the two leading techniques in the field of deep learning in medical imaging. We conducted a survey systematically. Particular attention was given to the robustness, computational efficiency, scalability, and accuracy of these models in handling complex medical datasets. The review incorporates findings from 36 studies and indicates a collective trend that transformer-based models, particularly ViTs, exhibit significant potential in diverse medical imaging tasks, showcasing superior performance when contrasted with conventional CNN models. Additionally, it is evident that pre-training is important for transformer applications. We expect this work to help researchers and practitioners select the most appropriate model for specific medical image analysis tasks, accounting for the current state of the art and future trends in the field.},
	language = {en},
	number = {1},
	urldate = {2024-12-02},
	journal = {Journal of Medical Systems},
	author = {Takahashi, Satoshi and Sakaguchi, Yusuke and Kouno, Nobuji and Takasawa, Ken and Ishizu, Kenichi and Akagi, Yu and Aoyama, Rina and Teraya, Naoki and Bolatkan, Amina and Shinkai, Norio and Machino, Hidenori and Kobayashi, Kazuma and Asada, Ken and Komatsu, Masaaki and Kaneko, Syuzo and Sugiyama, Masashi and Hamamoto, Ryuji},
	month = sep,
	year = {2024},
	keywords = {Artificial intelligence, Convolutional neural network, Medical Imaging, Medical image analysis, Prior learning, Vision transformer},
	pages = {84},
}

@article{javadi_novel_2019,
	title = {A novel deep learning method for automatic assessment of human sperm images},
	volume = {109},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482519301386},
	doi = {10.1016/j.compbiomed.2019.04.030},
	language = {en},
	urldate = {2024-12-15},
	journal = {Computers in Biology and Medicine},
	author = {Javadi, Soroush and Mirroshandel, Seyed Abolghasem},
	month = jun,
	year = {2019},
	pages = {182--194},
}

@article{chen_svia_2022,
	title = {{SVIA} dataset: {A} new dataset of microscopic videos and images for computer-aided sperm analysis},
	volume = {42},
	issn = {0208-5216},
	shorttitle = {{SVIA} dataset},
	url = {https://www.sciencedirect.com/science/article/pii/S0208521621001480},
	doi = {10.1016/j.bbe.2021.12.010},
	abstract = {Computer-Aided Sperm Analysis (CASA) is a widely studied topic in the diagnosis and treatment of male reproductive health. Although CASA has been evolving, there is still a lack of publicly available large-scale image datasets for CASA. To fill this gap, we provide the Sperm Videos and Images Analysis (SVIA) dataset, including three different subsets, subset-A, subset-B and subset-C, to test and evaluate different computer vision techniques in CASA. For subset-A, in order to test and evaluate the effectiveness of SVIA dataset for object detection, we use five representative object detection models and four commonly used evaluation metrics. For subset-B, in order to test and evaluate the effectiveness of SVIA dataset for image segmentation, we used eight representative methods and three standard evaluation metrics. Moreover, to test and evaluate the effectiveness of SVIA dataset for object tracking, we have employed the traditional kNN with progressive sperm (PR) as an evaluation metric and two deep learning models with three standard evaluation metrics. For subset-C, to prove the effectiveness of SVIA dataset for image denoising, nine denoising filters are used to denoise thirteen kinds of noise, and the mean structural similarity is calculated for evaluation. At the same time, to test and evaluate the effectiveness of SVIA dataset for image classification, we evaluate the results of twelve convolutional neural network models and six visual transformer models using four commonly used evaluation metrics. Through a series of experimental analyses and comparisons in this paper, it can be concluded that this proposed dataset can evaluate not only the functions of object detection, image segmentation, object tracking, image denoising, and image classification but also the robustness of object detection and image classification models. Therefore, SVIA dataset can fill the gap of the lack of large-scale public datasets in CASA and promote the development of CASA. Dataset is available at: https://github.com/Demozsj/Detection-Sperm.},
	number = {1},
	urldate = {2024-12-15},
	journal = {Biocybernetics and Biomedical Engineering},
	author = {Chen, Ao and Li, Chen and Zou, Shuojia and Rahaman, Md Mamunur and Yao, Yudong and Chen, Haoyuan and Yang, Hechen and Zhao, Peng and Hu, Weiming and Liu, Wanli and Grzegorzek, Marcin},
	month = jan,
	year = {2022},
	keywords = {Computer-aided sperm analysis, Medical image, Microscopic image, Microscopic video, Open dataset},
	pages = {204--214},
}

@article{drlik_qualitative_nodate,
	title = {Qualitative and quantitative semen parameters in the light  of the latest edition of the {WHO} manual for the examination  and processing of human semen},
	abstract = {Drlík M, Krátká Z, Novák J. Qualitative and quantitative semen parameters in the light of the latest edition of the WHO manual for the examination and processing of human semen.},
	language = {cs},
	author = {Drlík, Marcel and Krátká, Zuzana and Novák, Jan},
}

@article{singh_thakur_deep_2022,
	title = {Deep transfer learning based photonics sensor for assessment of seed-quality},
	volume = {196},
	issn = {01681699},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169922002083},
	doi = {10.1016/j.compag.2022.106891},
	language = {en},
	urldate = {2022-07-27},
	journal = {Computers and Electronics in Agriculture},
	author = {Singh Thakur, Puneet and Tiwari, Bhavya and Kumar, Abhishek and Gedam, Bhavesh and Bhatia, Vimal and Krejcar, Ondrej and Dobrovolny, Michal and Nebhen, Jamel and Prakash, Shashi},
	month = may,
	year = {2022},
	pages = {106891},
}

@article{sharma_hyperparameter-free_2023,
	title = {Hyperparameter-{Free} {RFF} {Based} {Post}-{Distorter} for {OTFS} {VLC} {System}},
	volume = {PP},
	doi = {10.1109/JPHOT.2023.3263560},
	abstract = {Visible light communication (VLC) has emerged as an eco-friendly and low-cost technology for the next-generation communication systems. However, for VLC systems, existing works report performance degradation due to light-emitting-diode (LED) non-linearity, multipath and relative mobility between the transmitter and the receiver. Additionally, multipath and user-mobility introduce inter-symbol-interference (ISI) and frequency-domain spreading which degrades the performance of VLC systems. For such scenarios, orthogonal time frequency space (OTFS) modulation is well-known to jointly addresses impairments due to multipath and user-mobility. To mitigate the distortions due to LED non-linearity, recently reproducing kernel Hilbert space (RKHS)-based random Fourier feature (RFF) techniques have emerged which alleviate the dependence on learning a dictionary and, outperforms classical polynomial based techniques. However, performance of these techniques is sensitive to the choice of kernel-width. Thus, for the OTFS systems impaired by LED non-linearity, this manuscript proposes a hyperparameter-free RFF-based post-distorter, under finite-memory budget without the need of explicitly tuning kernel-width for best performance. Analytical bounds for the bit-error-rate (BER) performance of the proposed post-distorter are presented, and validated via simulations over realistic VLC channels. From the results it is verified that the proposed receiver achieves better BER performance over uncompensated scenario and classical baseline polynomial based technique.},
	journal = {IEEE Photonics Journal},
	author = {Sharma, Anupma and Mitra, Rangeet and Krejcar, Ondrej and Choi, Kwonhue and Dobrovolný, Michal and Bhatia, Vimal},
	month = apr,
	year = {2023},
	pages = {1--7},
}

@article{s_wireless_2022,
	title = {Wireless {Edge} {Caching} and {Content} {Popularity} {Prediction} using {Machine} {Learning}},
	volume = {PP},
	doi = {10.1109/MCE.2022.3160585},
	abstract = {The ever pervasive growth in information services and technology has resulted in the outbreak of demand for data in the wireless networks. This has made the network operators to ponder over the imminent difficulties such as computing capabilities and fronthaul-backhaul link capacities. Hence, to bridge the gap between the cloud capacity and the requirement of mobile services by the network edges, edge computing and caching techniques have been gaining more and more attention from researchers across the world. Further, motivated by the successful applications of ML in solving complex and dynamic problems, in this article, it has been used to advance edge caching capabilities. The proposed ML based algorithms have been evaluated and proved to have better performance compared with the existing conventional algorithms. The MSE, for the proposed DL algorithm, is 20-times less than the existing algorithms and while comparing with the simple neural network, the gain in MSE for the proposed DL algorithm is observed around 27\%. Similarly for the FL based caching algorithm, average cache hit gain is of the order of 10{\textasciicircum}4, hence demonstrating the benefit of the proposed algorithms. Additionally, opportunities for a promising upcoming future of ML in edge computing prediction have been discussed.},
	journal = {IEEE Consumer Electronics Magazine},
	author = {S., Krishnendu and Bharath, B. and Bhatia, Vimal and Jamel, Nebhen and Dobrovolný, Michal and Ratnarajah, T.},
	month = jan,
	year = {2022},
	pages = {1--1},
}

@incollection{rojas_novel_2020,
	address = {Cham},
	title = {Novel {Thermal} {Image} {Classification} {Based} on {Techniques} {Derived} from {Mathematical} {Morphology}: {Case} of {Breast} {Cancer}},
	volume = {12108},
	isbn = {978-3-030-45384-8 978-3-030-45385-5},
	shorttitle = {Novel {Thermal} {Image} {Classification} {Based} on {Techniques} {Derived} from {Mathematical} {Morphology}},
	url = {http://link.springer.com/10.1007/978-3-030-45385-5_61},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {Bioinformatics and {Biomedical} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Mambou, Sebastien and Krejcar, Ondrej and Selamat, Ali and Dobrovolny, Michal and Maresova, Petra and Kuca, Kamil},
	editor = {Rojas, Ignacio and Valenzuela, Olga and Rojas, Fernando and Herrera, Luis Javier and Ortuño, Francisco},
	year = {2020},
	doi = {10.1007/978-3-030-45385-5_61},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {683--694},
}

@inproceedings{maci_empirical_2022,
	title = {An {Empirical} {Test} of {Stacked} {Autoencoder} as {Recommendation} {Model}},
	url = {http://hdl.handle.net/20.500.12603/653},
	doi = {10.36689/uhk/hed/2022-01-038},
	urldate = {2022-10-23},
	author = {Langer, Jaroslav and Dobrovolny, Michal and Krejcar, Ondrej},
	editor = {Maci, Jan and Maresova, Petra and Firlej, Krzysztof and Soukal, Ivan},
	month = jun,
	year = {2022},
	pages = {391--399},
}

@incollection{kopecky_cycle_2022,
	title = {Cycle {Route} {Signs} {Detection} {Using} {Deep} {Learning}},
	isbn = {978-3-031-16013-4},
	abstract = {This article addresses the issue of detecting traffic signs signalling cycle routes. It is also necessary to read the number or text of the cycle route from the given image. These tags are kept under the identifier IS21 and have a defined, uniform design with text in the middle of the tag. The detection was solved using the You Look Only Once (YOLO) model, which works on the principle of a convolutional neural network. The OCR tool PythonOCR was used to read characters from tags. The success rate of IS21 tag detection is 93.4\%, and the success rate of reading text from tags is equal to 85.9\%. The architecture described in the article is suitable for solving the defined problem.KeywordsYOLOv5YOLOOCRObject detectionMachine learningComputer vision},
	author = {Kopecky, Lukas and Dobrovolný, Michal and Fuchs, Antonin and Selamat, Ali and Krejcar, Ondrej},
	month = sep,
	year = {2022},
	doi = {10.1007/978-3-031-16014-1_8},
	pages = {82--94},
}

@inproceedings{maci_forecasting_2021,
	title = {Forecasting of the {Stock} {Price} {Using} {Recurrent} {Neural} {Network} – {Long} {Short}-term {Memory}},
	url = {http://hdl.handle.net/20.500.12603/492},
	doi = {10.36689/uhk/hed/2021-01-014},
	urldate = {2022-07-27},
	author = {Dobrovolny, Michal and Soukal, Ivan and Salamat, Ali and Cierniak-Emerych, Anna and Krejcar, Ondrej},
	editor = {Maci, Jan and Maresova, Petra and Firlej, Krzysztof and Soukal, Ivan},
	month = mar,
	year = {2021},
	pages = {145--154},
}

@inproceedings{maresova_forecasting_2020,
	title = {Forecasting of {FOREX} {Price} {Trend} using {Recurrent} {Neural} {Network} - {Long} short-term memory},
	url = {http://hdl.handle.net/20.500.12603/212},
	doi = {10.36689/uhk/hed/2020-01-011},
	abstract = {This paper describes the use of Long short-term memory (LSTM) for FOREX pair EUR/USD price prediction. Aim of the paper is to test and proposes the best time block to predict based on a daily FOREX data. We employ the mean of absolute errors and the least mean squared errors to assess prediction results in order to find the time block. The best performing network has been trained for 30 day period and 100 epochs. This paper also describes the effect of training for a high number of epochs. Training dataset contained data from 1.4.1971 until 9.5.2019.},
	language = {en},
	urldate = {2020-05-28},
	author = {Dobrovolny, Michal and Soukal, Ivan and Lim, Kok Cheng and Selamat, Ali and Krejcar, Ondrej},
	editor = {Maresova, Petra and Jedlicka, Pavel and Firlej, Krzysztof and Soukal, Ivan},
	month = apr,
	year = {2020},
	pages = {95--103},
}

@incollection{nguyen_session_2021,
	address = {Cham},
	title = {Session {Based} {Recommendations} {Using} {Recurrent} {Neural} {Networks} - {Long} {Short}-{Term} {Memory}},
	volume = {12672},
	isbn = {978-3-030-73279-0 978-3-030-73280-6},
	url = {http://link.springer.com/10.1007/978-3-030-73280-6_5},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {Intelligent {Information} and {Database} {Systems}},
	publisher = {Springer International Publishing},
	author = {Dobrovolny, Michal and Selamat, Ali and Krejcar, Ondrej},
	editor = {Nguyen, Ngoc Thanh and Chittayasothorn, Suphamit and Niyato, Dusit and Trawiński, Bogdan},
	year = {2021},
	doi = {10.1007/978-3-030-73280-6_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {53--65},
}

@incollection{rojas_medical_2020,
	address = {Cham},
	title = {Medical {Image} {Data} {Upscaling} with {Generative} {Adversarial} {Networks}},
	volume = {12108},
	isbn = {978-3-030-45384-8 978-3-030-45385-5},
	url = {http://link.springer.com/10.1007/978-3-030-45385-5_66},
	abstract = {Super-resolution is one of the frequently investigated methods of image processing. The quality of the results is a constant problem in the methods used to obtain high resolution images. Interpolationbased methods have blurry output problems, while non-interpolation methods require a lot of training data and high computing power. In this paper, we present a supervised generative adversarial network system that accurately generates high resolution images from a low resolution input while maintaining pathological invariance. The proposed solution is optimized for small sets of input data. Compared to existing models, our network also provides faster learning. Another advantage of our approach is its versatility for various types of medical imaging methods. We used peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) as the output image quality evaluation method. The results of our test show an improvement of 5.76\% compared to optimizer Adam used in the original paper [10]. For faster training of the neural network model, calculations on the graphic card with the CUDA architecture were used.},
	language = {en},
	urldate = {2020-05-28},
	booktitle = {Bioinformatics and {Biomedical} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Dobrovolny, Michal and Mls, Karel and Krejcar, Ondrej and Mambou, Sebastien and Selamat, Ali},
	editor = {Rojas, Ignacio and Valenzuela, Olga and Rojas, Fernando and Herrera, Luis Javier and Ortuño, Francisco},
	year = {2020},
	doi = {10.1007/978-3-030-45385-5_66},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {739--749},
}

@incollection{wojtkiewicz_session_2021,
	address = {Cham},
	title = {Session {Based} {Recommendations} {Using} {Char}-{Level} {Recurrent} {Neural} {Networks}},
	volume = {1463},
	isbn = {978-3-030-88112-2 978-3-030-88113-9},
	url = {https://link.springer.com/10.1007/978-3-030-88113-9_3},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {Advances in {Computational} {Collective} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Dobrovolny, Michal and Langer, Jaroslav and Selamat, Ali and Krejcar, Ondrej},
	editor = {Wojtkiewicz, Krystian and Treur, Jan and Pimenidis, Elias and Maleszka, Marcin},
	year = {2021},
	doi = {10.1007/978-3-030-88113-9_3},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {30--41},
}

@misc{noauthor_world_2022,
	title = {World {Bank} {Open} {Data}},
	url = {https://data.worldbank.org},
	abstract = {Free and open access to global development data},
	language = {en},
	urldate = {2024-10-03},
	journal = {World Bank Open Data},
	year = {2022},
}

@article{zou_tod-cnn_2022,
	title = {{TOD}-{CNN}: {An} effective convolutional neural network for tiny object detection in sperm videos},
	volume = {146},
	issn = {00104825},
	shorttitle = {{TOD}-{CNN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482522003353},
	doi = {10.1016/j.compbiomed.2022.105543},
	language = {en},
	urldate = {2022-11-24},
	journal = {Computers in Biology and Medicine},
	author = {Zou, Shuojia and Li, Chen and Sun, Hongzan and Xu, Peng and Zhang, Jiawei and Ma, Pingli and Yao, Yudong and Huang, Xinyu and Grzegorzek, Marcin},
	month = jul,
	year = {2022},
	pages = {105543},
}

@article{you_machine_2021,
	title = {Machine learning for sperm selection},
	volume = {18},
	issn = {1759-4812, 1759-4820},
	url = {https://www.nature.com/articles/s41585-021-00465-1},
	doi = {10.1038/s41585-021-00465-1},
	language = {en},
	number = {7},
	urldate = {2024-06-22},
	journal = {Nature Reviews Urology},
	author = {You, Jae Bem and McCallum, Christopher and Wang, Yihe and Riordon, Jason and Nosrati, Reza and Sinton, David},
	month = jul,
	year = {2021},
	pages = {387--403},
}

@article{yang_multidimensional_2024,
	title = {Multidimensional morphological analysis of live sperm based on multiple-target tracking},
	volume = {24},
	issn = {20010370},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2001037024000497},
	doi = {10.1016/j.csbj.2024.02.025},
	language = {en},
	urldate = {2024-06-22},
	journal = {Computational and Structural Biotechnology Journal},
	author = {Yang, Hao and Ma, Mengmeng and Chen, Xiangfeng and Chen, Guowu and Shen, Yi and Zhao, Lijun and Wang, Jianfeng and Yan, Feifei and Huang, Difeng and Gao, Huijie and Jiang, Hao and Zheng, Yuqian and Wang, Yu and Xiao, Qian and Chen, Ying and Zhou, Jian and Shi, Jie and Guo, Yi and Liang, Bo and Teng, Xiaoming},
	month = dec,
	year = {2024},
	pages = {176--184},
}

@incollection{wojciechowski_application_2006,
	address = {Dordrecht},
	title = {{APPLICATION} {OF} {IMAGE} {PROCESSING} {TECHNIQUES} {IN} {MALE} {FERTILITY} {ASSESSMENT}},
	volume = {32},
	isbn = {978-1-4020-4178-5},
	url = {http://link.springer.com/10.1007/1-4020-4179-9_129},
	language = {en},
	urldate = {2024-06-22},
	booktitle = {Computer {Vision} and {Graphics}},
	publisher = {Kluwer Academic Publishers},
	author = {Witkowski, Lukasz and Rokita, Przemyslaw},
	editor = {Wojciechowski, K. and Smolka, B. and Palus, H. and Kozera, R.S. and Skarbek, W. and Noakes, L.},
	year = {2006},
	doi = {10.1007/1-4020-4179-9_129},
	note = {Series Title: Computational Imaging and Vision},
	pages = {881--887},
}

@misc{wang_testing_2024,
	title = {Testing the reproducibility and effectiveness of deep learning models among clinics: sperm detection as a pilot study},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Testing the reproducibility and effectiveness of deep learning models among clinics},
	url = {https://www.researchsquare.com/article/rs-4008354/v1},
	doi = {10.21203/rs.3.rs-4008354/v1},
	abstract = {Abstract
          Background:
Deep learning has been increasingly investigated for assisting clinical in vitro fertilization (IVF). The first technical step in many tasks is to visually detect and locate sperm, oocytes, and embryos in images. For clinical deployment of such deep learning models, different clinics use different image acquisition hardware and different sample preprocessing protocols, raising the concern over whether the reported accuracy of a deep learning model by one clinic could be reproduced in another clinic. Here we aim to investigate the effect of each imaging factor on the reproducibility of object detection models, using sperm analysis as a pilot example.
Methods:
Ablation studies were performed using state-of-the-art models for detecting human sperm to quantitatively assess how model precision (false-positive detection) and recall (missed detection) were affected by imaging magnification, imaging mode, and sample preprocessing protocols. The results led to the hypothesis that the richness of image acquisition conditions in a training dataset deterministically affects model reproducibility. The hypothesis was tested by first enriching the training dataset with a wide range of imaging conditions, then validated through internal blind tests on new samples and external multi-center clinical validations.
Results:
Ablation experiments revealed that removing subsets of data from the training dataset significantly reduced model precision. Removing raw sample images from the training dataset caused the largest drop in model precision, whereas removing 20x images caused the largest drop in model recall. by incorporating different imaging and sample preprocessing conditions into a rich training dataset, the model achieved an intraclass correlation coefficient (ICC) of 0.97 (95\% CI: 0.94-0.99) for precision, and an ICC of 0.97 (95\% CI: 0.93-0.99) for recall. Multi-center clinical validation showed no significant differences in model precision or recall across different clinics and applications.
Conclusions:
The results validated the hypothesis that the richness of data in the training dataset is a key factor impacting model reproducibility. These findings highlight the importance of diversity in a training dataset for model evaluation and suggest that future deep learning models in andrology and reproductive medicine should incorporate comprehensive feature sets for enhanced reproducibility across clinics.},
	urldate = {2024-06-22},
	author = {Wang, Jiaqi and Jin, Yufei and Jiang, Aojun and Chen, Wenyuan and Shan, Guanqiao and Gu, Yifan and Ming, Yue and Li, Jichang and Yue, Chunfeng and Huang, Zongjie and Librach, Clifford and Lin, Ge and Wang, Xibu and Zhao, Huan and Sun, Yu and Zhang, Zhuoran},
	month = mar,
	year = {2024},
}

@article{urbano_automatic_2017,
	title = {Automatic {Tracking} and {Motility} {Analysis} of {Human} {Sperm} in {Time}-{Lapse} {Images}},
	volume = {36},
	issn = {1558-254X},
	doi = {10.1109/TMI.2016.2630720},
	abstract = {We present a fully automated multi-sperm tracking algorithm. It has the demonstrated capability to detect and track simultaneously hundreds of sperm cells in recorded videos while accurately measuring motility parameters over time and with minimal operator intervention. Algorithms of this kind may help in associating dynamic swimming parameters of human sperm cells with fertility and fertilization rates. Specifically, we offer an image processing method, based on radar tracking algorithms, that detects and tracks automatically the swimming paths of human sperm cells in timelapse microscopy image sequences of the kind that is analyzed by fertility clinics. Adapting the well-known joint probabilistic data association filter (JPDAF), we automatically tracked hundreds of human sperm simultaneously and measured their dynamic swimming parameters over time. Unlike existing CASA instruments, our algorithm has the capability to track sperm swimming in close proximity to each other and during apparent cell-to-cell collisions. Collecting continuously parameters for each sperm tracked without sample dilution (currently impossible using standard CASA systems) provides an opportunity to compare such data with standard fertility rates. The use of our algorithm thus has the potential to free the clinician from having to rely on elaborate motility measurements obtained manually by technicians, speed up semen processing, and provide medical practitioners and researchers with more useful data than are currently available.},
	number = {3},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Urbano, Leonardo F. and Masson, Puneet and VerMilyea, Matthew and Kam, Moshe},
	month = mar,
	year = {2017},
	keywords = {Algorithm design and analysis, Computer assisted semen analysis (CASA), Head, Heuristic algorithms, Instruments, JPDAF, Radar tracking, Target tracking, Videos, human sperm imaging, sperm motility, sperm tracking},
	pages = {792--801},
}

@article{tumuklu_ozyer_hybrid_2022,
	title = {A hybrid {IMM}-{JPDAF} algorithm for tracking multiple sperm targets and motility analysis},
	volume = {34},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-022-07390-3},
	doi = {10.1007/s00521-022-07390-3},
	abstract = {Semen analysis has received a lot of attention because of its important role in determining infertility in men. It involves several factors, the most important of which are sperm morphology, sperm concentration, and sperm motility. In addition, measurements of sperm cell mobility reflect important parameters in medical diagnosis. As computer-assisted semen analysis systems are very expensive and not prolific, especially in small medical laboratories, semen analysis is often done manually. This is a time-consuming and costly process. Therefore, we have developed an automated system that evaluates sperm motility parameters which can be of great help to clinicians in achieving more accurate results at a lower cost and time. The system tracks the movement of most spermatozoa cells in a semen sample taken from a microscope and then carefully measures all the parameters related to sperm movement to determine the fertility or infertility rate of the subject. Detecting large numbers of sperm cells is challenging because there are a large number of colliding targets that cause false alarms. In this work, the background subtraction method is used to determine the sperms within the video frames, and the joint probabilistic data association filter algorithm is used to estimate the sperm trajectory and to associate different tracks. Since the sperm represents maneuvering movements, the interacting multiple models technique was used along with the JPDAF algorithms to obtain more accurate results. Our evaluations on real and synthetic data reveal the superiority of our method over previous work in sperm cell tracking.},
	language = {en},
	number = {20},
	urldate = {2023-01-03},
	journal = {Neural Computing and Applications},
	author = {Tumuklu Ozyer, Gulsah and Ozyer, Baris and Negin, Farhood and Alarabi, İnas and Agahian, Saeid},
	month = oct,
	year = {2022},
	keywords = {Data association, Interacting multiple models, Multi-target tracking, Sperm motility, Video analysis},
	pages = {17407--17421},
}

@book{torralba_foundations_2024,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning series},
	title = {Foundations of computer vision},
	isbn = {978-0-262-37867-3 978-0-262-37866-6},
	abstract = {"An up-to-date computer vision textbook incorporating the latest deep learning advances that have revolutionized the field over the last decade"--},
	publisher = {The MIT Press},
	author = {Torralba, Antonio},
	year = {2024},
	keywords = {Computer vision},
}

@article{wu_preliminary_2021,
	title = {A preliminary study of sperm identification in microdissection testicular sperm extraction samples with deep convolutional neural networks},
	volume = {23},
	issn = {1008-682X},
	url = {https://www.ajandrology.com/article.asp?issn=1008-682X;year=2021;volume=23;issue=2;spage=135;epage=139;aulast=Wu},
	doi = {10.4103/aja.aja_66_20},
	language = {en},
	number = {2},
	urldate = {2022-11-24},
	journal = {Asian Journal of Andrology},
	author = {Wu, DanielJ and Badamjav, Odgerel and Reddy, VikrantV and Eisenberg, Michael and Behr, Barry},
	year = {2021},
	pages = {135},
}

@misc{who_1_2023,
	title = {1 in 6 people globally affected by infertility: {WHO}},
	shorttitle = {1 in 6 people globally affected by infertility},
	url = {https://www.who.int/news/item/04-04-2023-1-in-6-people-globally-affected-by-infertility},
	abstract = {Large numbers of people are affected by infertility in their lifetime, according to a new report published today by WHO – showing the urgent need to increase access to affordable, high-quality fertility care for those in need.},
	language = {en},
	urldate = {2024-10-03},
	author = {WHO},
	year = {2023},
}

@article{suleman_review_2023,
	title = {A review of different deep learning techniques for sperm fertility prediction},
	volume = {8},
	issn = {2473-6988},
	url = {http://www.aimspress.com/article/doi/10.3934/math.2023838},
	doi = {10.3934/math.2023838},
	abstract = {{\textless}abstract{\textgreater}
			{\textless}p{\textgreater}Sperm morphology analysis (SMA) is a significant factor in diagnosing male infertility. Therefore, healthy sperm detection is of great significance in this process. However, the traditional manual microscopic sperm detection methods have the disadvantages of a long detection cycle, low detection accuracy in large orders, and very complex fertility prediction. Therefore, it is meaningful to apply computer image analysis technology to the field of fertility prediction. Computer image analysis can give high precision and high efficiency in detecting sperm cells. In this article, first, we analyze the existing sperm detection techniques in chronological order, from traditional image processing and machine learning to deep learning methods in segmentation and classification. Then, we analyze and summarize these existing methods and introduce some potential methods, including visual transformers. Finally, the future development direction and challenges of sperm cell detection are discussed. We have summarized 44 related technical papers from 2012 to the present. This review will help researchers have a more comprehensive understanding of the development process, research status, and future trends in the field of fertility prediction and provide a reference for researchers in other fields.{\textless}/p{\textgreater}
		{\textless}/abstract{\textgreater}},
	number = {7},
	urldate = {2024-06-22},
	journal = {AIMS Mathematics},
	author = {Suleman, Muhammad and Ilyas, Muhammad and Lali, M. Ikram Ullah and Rauf, Hafiz Tayyab and Kadry, Seifedine},
	year = {2023},
	pages = {16360--16416},
}

@misc{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	doi = {10.48550/arXiv.1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	urldate = {2023-12-10},
	publisher = {arXiv},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {arXiv:1506.02640 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@book{peters_computational_2016,
	address = {Cham},
	series = {Intelligent systems reference library},
	title = {Computational proximity: excursions in the topology of digital images},
	isbn = {978-3-319-30262-1 978-3-319-30260-7},
	shorttitle = {Computational proximity},
	language = {eng},
	number = {102},
	publisher = {Springer},
	author = {Peters, James F.},
	year = {2016},
}

@article{shi_computer-based_2006,
	title = {Computer-based tracking of single sperm},
	volume = {11},
	issn = {10833668},
	url = {http://biomedicaloptics.spiedigitallibrary.org/article.aspx?doi=10.1117/1.2357735},
	doi = {10.1117/1.2357735},
	language = {en},
	number = {5},
	urldate = {2024-06-22},
	journal = {Journal of Biomedical Optics},
	author = {Shi, Linda Z. and Nascimento, Jaclyn M. and Berns, Michael W. and Botvinick, Elliot L.},
	year = {2006},
	pages = {054009},
}

@misc{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	doi = {10.48550/arXiv.1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv:1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{page_prisma_2021,
	title = {The {PRISMA} 2020 statement: {An} updated guideline for reporting systematic reviews},
	volume = {18},
	issn = {1549-1676},
	shorttitle = {The {PRISMA} 2020 statement},
	url = {https://dx.plos.org/10.1371/journal.pmed.1003583},
	doi = {10.1371/journal.pmed.1003583},
	language = {en},
	number = {3},
	urldate = {2024-06-22},
	journal = {PLOS Medicine},
	author = {Page, Matthew J. and McKenzie, Joanne E. and Bossuyt, Patrick M. and Boutron, Isabelle and Hoffmann, Tammy C. and Mulrow, Cynthia D. and Shamseer, Larissa and Tetzlaff, Jennifer M. and Akl, Elie A. and Brennan, Sue E. and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M. and Hróbjartsson, Asbjørn and Lalu, Manoj M. and Li, Tianjing and Loder, Elizabeth W. and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A. and Stewart, Lesley A. and Thomas, James and Tricco, Andrea C. and Welch, Vivian A. and Whiting, Penny and Moher, David},
	month = mar,
	year = {2021},
	pages = {e1003583},
}

@incollection{sharma_convolutional_2017,
	address = {Cham},
	title = {Convolutional {Neural} {Networks} for {Segmentation} and {Object} {Detection} of {Human} {Semen}},
	volume = {10269},
	isbn = {978-3-319-59125-4 978-3-319-59126-1},
	url = {https://link.springer.com/10.1007/978-3-319-59126-1_33},
	language = {en},
	urldate = {2022-11-23},
	booktitle = {Image {Analysis}},
	publisher = {Springer International Publishing},
	author = {Nissen, Malte S. and Krause, Oswin and Almstrup, Kristian and Kjærulff, Søren and Nielsen, Torben T. and Nielsen, Mads},
	editor = {Sharma, Puneet and Bianchi, Filippo Maria},
	year = {2017},
	doi = {10.1007/978-3-319-59126-1_33},
	pages = {397--406},
}

@article{miahi_genetic_2022,
	title = {Genetic {Neural} {Architecture} {Search} for automatic assessment of human sperm images},
	volume = {188},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421012914},
	doi = {10.1016/j.eswa.2021.115937},
	language = {en},
	urldate = {2022-11-24},
	journal = {Expert Systems with Applications},
	author = {Miahi, Erfan and Mirroshandel, Seyed Abolghasem and Nasr, Alexis},
	month = feb,
	year = {2022},
	pages = {115937},
}

@inproceedings{mambou_novel_2020,
	address = {Cham},
	title = {Novel {Thermal} {Image} {Classification} {Based} on {Techniques} {Derived} from {Mathematical} {Morphology}: {Case} of {Breast} {Cancer}},
	isbn = {978-3-030-45385-5},
	shorttitle = {Novel {Thermal} {Image} {Classification} {Based} on {Techniques} {Derived} from {Mathematical} {Morphology}},
	doi = {10.1007/978-3-030-45385-5_61},
	abstract = {Image processing (IP) is a method of converting an image to digital form by performing operations on it to obtain an improved image or to extract useful information from it. It is a type of signal distribution in which the input is an image such as a video image or a photograph, and the output can be an image or associated characteristics. Besides, this system includes the processing of images in the form of two-dimensional signals, while applying signal processing methods already defined for them. One of the essential steps in IP is a combined application of erosion and dilation procedures, which is part of the mathematical morphology. This article presents a novel thermal image classification based on techniques derived from mathematical morphology. In the processing of grayscale breast cancer images, this method reveals the region of interest as the whitest area.},
	language = {en},
	booktitle = {Bioinformatics and {Biomedical} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Mambou, Sebastien and Krejcar, Ondrej and Selamat, Ali and Dobrovolny, Michal and Maresova, Petra and Kuca, Kamil},
	editor = {Rojas, Ignacio and Valenzuela, Olga and Rojas, Fernando and Herrera, Luis Javier and Ortuño, Francisco},
	year = {2020},
	pages = {683--694},
}

@article{pan_sperm_2022,
	title = {A {Sperm} {Quality} {Detection} {System} {Based} on {Microfluidic} {Chip} and {Micro}-{Imaging} {System}},
	volume = {9},
	issn = {2297-1769},
	url = {https://www.frontiersin.org/articles/10.3389/fvets.2022.916861/full},
	doi = {10.3389/fvets.2022.916861},
	abstract = {Sperm quality assessment is the main method to predict the reproductive ability of livestock. The detection of sperm quality of livestock is of great significance to the application of artificial insemination and 
              in vitro 
              fertilization. In order to comprehensively evaluate sperm quality and improve the real-time and portability of sperm quality detection, a portable microscopic imaging system based on microfluidic chip is developed in this paper. The system can realize the comprehensive evaluation of sperm quality by detecting sperm vitality and survival rate. On the hardware side, a microfluidic chip is designed, which can automatically mix samples. A set of optical system with a magnification of 400 times was developed for microscopic observation of sperm. In the aspect of software, aiming at the comprehensive evaluation of sperm quality based on OpenCV, a set of algorithms for identifying sperm motility and survival rate is proposed. The accuracy of the system in detecting sperm survival rate is 94.0\%, and the error rate is 0.6\%. The evaluation results of sperm motility are consistent with those of computer-aided sperm analysis (CASA). The system's identification time is 9 s. Therefore, the system is absolutely suitable for sperm quality detection.},
	urldate = {2022-11-24},
	journal = {Frontiers in Veterinary Science},
	author = {Pan, Xiaoqing and Gao, Kang and Yang, Ning and Wang, Yafei and Zhang, Xiaodong and Shao, Le and Zhai, Pin and Qin, Feng and Zhang, Xia and Li, Jian and Wang, Xinglong and Yang, Jie},
	month = jun,
	year = {2022},
	pages = {916861},
}

@article{matsuzaki_sperm_2022,
	title = {Sperm {Motility} {Regulation} in {Male} and {Female} {Bird} {Genital} {Tracts}},
	volume = {59},
	issn = {1346-7395, 1349-0486},
	url = {https://www.jstage.jst.go.jp/article/jpsa/59/1/59_0200105/_article},
	doi = {10.2141/jpsa.0200105},
	language = {en},
	number = {1},
	urldate = {2024-06-22},
	journal = {The Journal of Poultry Science},
	author = {Matsuzaki, Mei and Sasanami, Tomohiro},
	year = {2022},
	pages = {1--7},
}

@article{mashaal_automatic_2022,
	title = {Automatic {Healthy} {Sperm} {Head} {Detection} using {Deep} {Learning}},
	volume = {13},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=13&Issue=4&Code=IJACSA&SerialNo=86},
	doi = {10.14569/IJACSA.2022.0130486},
	language = {en},
	number = {4},
	urldate = {2022-11-24},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Mashaal, Ahmad Abdelaziz and Eldosoky, Mohamed A. A. and Mahdy, Lamia Nabil and Ezzat, Kadry Ali},
	year = {2022},
}

@inproceedings{liu_ssd_2016,
	address = {Cham},
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	isbn = {978-3-319-46448-0},
	shorttitle = {{SSD}},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For \$\$300 {\textbackslash}times 300\$\$300×300input, SSD achieves 74.3 \% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for \$\$512 {\textbackslash}times 512\$\$512×512input, SSD achieves 76.9 \% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Convolutional neural network, Real-time object detection},
	pages = {21--37},
}

@misc{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	doi = {10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2023-12-10},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv:1405.0312 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{leung_ai-based_2019,
	title = {{AI}-{Based} {Sensor} {Information} {Fusion} for {Supporting} {Deep} {Supervised} {Learning}},
	volume = {19},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/6/1345},
	doi = {10.3390/s19061345},
	abstract = {In recent years, artificial intelligence (AI) and its subarea of deep learning have drawn the attention of many researchers. At the same time, advances in technologies enable the generation or collection of large amounts of valuable data (e.g., sensor data) from various sources in different applications, such as those for the Internet of Things (IoT), which in turn aims towards the development of smart cities. With the availability of sensor data from various sources, sensor information fusion is in demand for effective integration of big data. In this article, we present an AI-based sensor-information fusion system for supporting deep supervised learning of transportation data generated and collected from various types of sensors, including remote sensed imagery for the geographic information system (GIS), accelerometers, as well as sensors for the global navigation satellite system (GNSS) and global positioning system (GPS). The discovered knowledge and information returned from our system provides analysts with a clearer understanding of trajectories or mobility of citizens, which in turn helps to develop better transportation models to achieve the ultimate goal of smarter cities. Evaluation results show the effectiveness and practicality of our AI-based sensor information fusion system for supporting deep supervised learning of big transportation data.},
	language = {en},
	number = {6},
	urldate = {2024-06-22},
	journal = {Sensors},
	author = {Leung, Carson K. and Braun, Peter and Cuzzocrea, Alfredo},
	month = mar,
	year = {2019},
	pages = {1345},
}

@article{ilhan_smartphone_2020,
	title = {Smartphone based sperm counting - an alternative way to the visual assessment technique in sperm concentration analysis},
	volume = {79},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-019-08421-3},
	doi = {10.1007/s11042-019-08421-3},
	language = {en},
	number = {9-10},
	urldate = {2024-06-22},
	journal = {Multimedia Tools and Applications},
	author = {Ilhan, Hamza Osman and Aydin, Nizamettin},
	month = mar,
	year = {2020},
	pages = {6409--6435},
}

@article{lopez_florez_automatic_2023,
	title = {Automatic {Cell} {Counting} {With} {YOLOv5}: {A} {Fluorescence} {Microscopy} {Approach}},
	volume = {8},
	issn = {1989-1660},
	shorttitle = {Automatic {Cell} {Counting} {With} {YOLOv5}},
	url = {https://www.ijimai.org/journal/sites/default/files/2023-08/ijimai8_3_6.pdf},
	doi = {10.9781/ijimai.2023.08.001},
	language = {en},
	number = {3},
	urldate = {2024-06-22},
	journal = {International Journal of Interactive Multimedia and Artificial Intelligence},
	author = {López Flórez, Sebastián and González-Briones, Alfonso and Hernández, Guillermo and Ramos, Carlos and De La Prieta, Fernando},
	year = {2023},
	pages = {64},
}

@misc{huang_speedaccuracy_2017,
	title = {Speed/accuracy trade-offs for modern convolutional object detectors},
	url = {http://arxiv.org/abs/1611.10012},
	doi = {10.48550/arXiv.1611.10012},
	abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
	month = apr,
	year = {2017},
	note = {arXiv:1611.10012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{hidayatullah_deepsperm_2021,
	title = {{DeepSperm}: {A} robust and real-time bull sperm-cell detection in densely populated semen videos},
	volume = {209},
	issn = {0169-2607},
	shorttitle = {{DeepSperm}},
	url = {https://www.sciencedirect.com/science/article/pii/S016926072100376X},
	doi = {10.1016/j.cmpb.2021.106302},
	abstract = {Background and Objective
Object detection is a primary research interest in computer vision. Sperm-cell detection in a densely populated bull semen microscopic observation video presents challenges that are more difficult than those presented by other general object-detection cases. These challenges include partial occlusion, vast number of objects in a single video frame, tiny size of the object, artifacts, low contrast, low video resolution, and blurry objects because of the rapid movement of the sperm cells. This study proposes a deep neural network architecture, called DeepSperm, that solves the aforementioned problems and is more accurate and faster than state-of-the-art architectures.
Methods
In the proposed architecture, we use only one detection layer, which is specific for small object detection. For handling overfitting and increasing accuracy, we set a higher input network resolution, use a dropout layer, and perform data augmentation on saturation and exposure. Several hyper-parameters are tuned to achieve better performance. Mean average precision (mAP), confusion matrix, precision, recall, and F1-score are used to measure accuracy. Frame per second (fps) is used to measure speed. We compare our proposed method with you only look once (YOLO) v3 and YOLOv4.
Results
In our experiment, we achieve 94.11 mAP on the test dataset, F1-score of 0.93, and a processing speed of 51.9 fps. In comparison with YOLOv4, our proposed method is 2.18 x faster on testing, and 2.9 x faster on training with a small dataset, while achieving comparative detection accuracy. The weights file size was also reduced significantly, with one-twentieth that of YOLOv4. Moreover, it requires a 1.07 x less graphical processing unit (GPU) memory than YOLOv4.
Conclusions
This study proposes DeepSperm, which is a simple, effective, and efficient deep neural network architecture with its hyper-parameters and configuration to detect bull sperm cells robustly in real time. In our experiments, we surpass the state-of-the-art in terms of accuracy, speed, and resource needs.},
	language = {en},
	urldate = {2022-03-18},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Hidayatullah, Priyanto and Wang, Xueting and Yamasaki, Toshihiko and Mengko, Tati L. E. R. and Munir, Rinaldi and Barlian, Anggraini and Sukmawati, Eros and Supraptono, Supraptono},
	month = sep,
	year = {2021},
	keywords = {Computer-aided sperm analysis, Small-object detection, Sperm-cell detection, YOLO, notion},
	pages = {106302},
}

@article{hicks_machine_2019,
	title = {Machine {Learning}-{Based} {Analysis} of {Sperm} {Videos} and {Participant} {Data} for {Male} {Fertility} {Prediction}},
	volume = {9},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-53217-y},
	doi = {10.1038/s41598-019-53217-y},
	abstract = {Abstract
            Methods for automatic analysis of clinical data are usually targeted towards a specific modality and do not make use of all relevant data available. In the field of male human reproduction, clinical and biological data are not used to its fullest potential. Manual evaluation of a semen sample using a microscope is time-consuming and requires extensive training. Furthermore, the validity of manual semen analysis has been questioned due to limited reproducibility, and often high inter-personnel variation. The existing computer-aided sperm analyzer systems are not recommended for routine clinical use due to methodological challenges caused by the consistency of the semen sample. Thus, there is a need for an improved methodology. We use modern and classical machine learning techniques together with a dataset consisting of 85 videos of human semen samples and related participant data to automatically predict sperm motility. Used techniques include simple linear regression and more sophisticated methods using convolutional neural networks. Our results indicate that sperm motility prediction based on deep learning using sperm motility videos is rapid to perform and consistent. Adding participant data did not improve the algorithms performance. In conclusion, machine learning-based automatic analysis may become a valuable tool in male infertility investigation and research.},
	language = {en},
	number = {1},
	urldate = {2024-06-22},
	journal = {Scientific Reports},
	author = {Hicks, Steven A. and Andersen, Jorunn M. and Witczak, Oliwia and Thambawita, Vajira and Halvorsen, Pål and Hammer, Hugo L. and Haugen, Trine B. and Riegler, Michael A.},
	month = nov,
	year = {2019},
	pages = {16770},
}

@article{ilhan_fully_2020,
	title = {A fully automated hybrid human sperm detection and classification system based on mobile-net and the performance comparison with conventional methods},
	volume = {58},
	issn = {0140-0118, 1741-0444},
	url = {http://link.springer.com/10.1007/s11517-019-02101-y},
	doi = {10.1007/s11517-019-02101-y},
	language = {en},
	number = {5},
	urldate = {2022-11-23},
	journal = {Medical \& Biological Engineering \& Computing},
	author = {Ilhan, Hamza O. and Sigirci, I. Onur and Serbes, Gorkem and Aydin, Nizamettin},
	month = may,
	year = {2020},
	pages = {1047--1068},
}

@book{haugen_visem_2019,
	title = {{VISEM}: {A} {Multimodal} {Video} {Dataset} of {Human} {Spermatozoa}},
	copyright = {© Haugen, T. B., Hicks, S., Andersen, J. M., Witczak, O., Hammer, H. L., Borgli, R. J., Halvorsen, P., Riegler, M. {\textbar} ACM 2019. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in  MMSys '19: Proceedings of the 10th ACM Multimedia Systems Conference, http://dx.doi.org/10.1145/3304109.3325814.},
	isbn = {978-1-4503-6297-9},
	shorttitle = {{VISEM}},
	url = {https://oda.oslomet.no/oda-xmlui/handle/10642/8248},
	abstract = {Real multimedia datasets that contain more than just images or text are rare. Even more so are open multimedia datasets in medicine. Often, clinically related datasets only consist of image or videos. In this paper, we present a dataset that is novel in two ways. Firstly, it is a multi-modal dataset containing different data sources such as videos, biological analysis data, and participant data. Secondly, it is the first dataset of that kind in the field of human reproduction. It consists of anonymized data from 85 different participants. We hope this dataset paper will inspire people to apply their knowledge in this important field, generate shareable results in the domain, and ultimately improve human infertility investigation and treatment.},
	language = {en},
	urldate = {2023-12-10},
	publisher = {ACM Publications},
	author = {Haugen, Trine B. and Hicks, Steven and Andersen, Jorunn Marie and Witczak, Oliwia and Hammer, Hugo Lewi and Borgli, Rune Johan and Halvorsen, Pål and Riegler, Michael},
	year = {2019},
	doi = {10.1145/3304109.3325814},
	note = {Accepted: 2020-02-03T10:32:59Z
Publication Title: 978-1-4503-6297-9},
}

@article{gumuscu_estimation_2019,
	title = {Estimation of active sperm count in spermiogram using motion detection methods},
	volume = {34},
	issn = {1300-1884},
	url = {https://www.webofscience.com/wos/woscc/full-record/WOS:000469481500012},
	doi = {10.17341/gazimmfd.460524},
	abstract = {Nowadays, infertility is a serious health problem. The most important parameters that cause infertility in males are sperm morphology, sperm motility and sperm density. For this reason, sperm motility, density and morphology are analyzed in semen analysis and these analyzes are done by experts in the laboratory. Analysis based on observation in the laboratory is easy to mistake, subjective application. In this study, computer aided sperm count estimation method is suggested to minimize the effect of experts in semen analysis. In this study, especially the sperm motility is focused and the number of active sperm in the semen is estimated by using motion detection methods. Tests of the proposed method were performed with the data set prepared by the Isfahan Fertility and Infertility Center (IFIC). The experimental results obtained are compared with the previously proposed methods in the literature and it is seen that the proposed method gives successful results.},
	language = {Turkish},
	number = {3},
	urldate = {2022-03-20},
	journal = {Journal of the Faculty of Engineering and Architecture of Gazi University},
	author = {Gumuscu, Abdulkadir and Tenekeci, Mehmet Emin},
	year = {2019},
	note = {Place: Ankara
Publisher: Gazi Univ, Fac Engineering Architecture
WOS:000469481500012},
	keywords = {Semen analysis, classification, foreground detection, notion, sperm segmentation, time},
	pages = {1274--1280},
}

@article{gumuscu_spermiogram_2018,
	title = {Spermiogram {Görüntülerinden} {Hareket} {Belirleme} {Yöntemleri} ile {Aktif} {Sperm} {Sayısının} {Tahmini}},
	volume = {2018},
	issn = {1300-1884},
	url = {https://dergipark.org.tr/tr/doi/10.17341/gazimmfd.460524},
	doi = {10.17341/gazimmfd.460524},
	number = {18-2},
	urldate = {2024-06-22},
	journal = {Gazi Üniversitesi Mühendislik-Mimarlık Fakültesi Dergisi},
	author = {Gümüşçü, Abdülkadir and Tenekeci, Mehmet Emin},
	month = sep,
	year = {2018},
}

@misc{grill_bootstrap_2020,
	title = {Bootstrap your own latent: {A} new approach to self-supervised {Learning}},
	shorttitle = {Bootstrap your own latent},
	url = {http://arxiv.org/abs/2006.07733},
	doi = {10.48550/arXiv.2006.07733},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3{\textbackslash}\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6{\textbackslash}\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	month = sep,
	year = {2020},
	note = {arXiv:2006.07733},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{field_machine_2021,
	title = {Machine learning applications in radiation oncology},
	volume = {19},
	issn = {2405-6316},
	url = {https://www.phiro.science/article/S2405-6316(21)00030-0/fulltext},
	doi = {10.1016/j.phro.2021.05.007},
	language = {English},
	urldate = {2024-07-30},
	journal = {Physics and Imaging in Radiation Oncology},
	author = {Field, Matthew and Hardcastle, Nicholas and Jameson, Michael and Aherne, Noel and Holloway, Lois},
	month = jul,
	year = {2021},
	pmid = {34307915},
	note = {Publisher: Elsevier},
	keywords = {Artificial intelligence, Automation, Data mining, Machine learning, Radiation therapy},
	pages = {13--24},
}

@article{gil_predicting_2012,
	title = {Predicting seminal quality with artificial intelligence methods},
	volume = {39},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417412007269},
	doi = {10.1016/j.eswa.2012.05.028},
	language = {en},
	number = {16},
	urldate = {2024-06-22},
	journal = {Expert Systems with Applications},
	author = {Gil, David and Girela, Jose Luis and De Juan, Joaquin and Gomez-Torres, M. Jose and Johnsson, Magnus},
	month = nov,
	year = {2012},
	pages = {12564--12573},
}

@inproceedings{fraczek_sperm_2022,
	address = {Melbourne, Australia},
	title = {Sperm segmentation and abnormalities detection during the {ICSI} procedure using machine learning algorithms},
	isbn = {978-1-66546-822-0},
	url = {https://ieeexplore.ieee.org/document/9869511/},
	doi = {10.1109/HSI55341.2022.9869511},
	urldate = {2022-11-24},
	booktitle = {2022 15th {International} {Conference} on {Human} {System} {Interaction} ({HSI})},
	publisher = {IEEE},
	author = {Fraczek, Aleksandra and Karwowska, Gabriela and Miler, Mateusz and Lis, Joanna and Jezierska, Anna and Mazur-Milecka, Magdalena},
	month = jul,
	year = {2022},
	pages = {1--6},
}

@article{elsayed_development_2015,
	title = {Development of computer-assisted sperm analysis plugin for analyzing sperm motion in microfluidic environments using {Image}-{J}},
	volume = {84},
	issn = {0093691X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0093691X15003799},
	doi = {10.1016/j.theriogenology.2015.07.021},
	language = {en},
	number = {8},
	urldate = {2024-06-22},
	journal = {Theriogenology},
	author = {Elsayed, Mohamed and El-Sherry, Taymour M. and Abdelgawad, Mohamed},
	month = nov,
	year = {2015},
	pages = {1367--1377},
}

@article{chu_artificial_2019,
	title = {Artificial {Intelligence} in {Reproductive} {Urology}},
	volume = {20},
	issn = {1527-2737, 1534-6285},
	url = {http://link.springer.com/10.1007/s11934-019-0914-4},
	doi = {10.1007/s11934-019-0914-4},
	language = {en},
	number = {9},
	urldate = {2024-06-22},
	journal = {Current Urology Reports},
	author = {Chu, Kevin Y. and Nassau, Daniel E. and Arora, Himanshu and Lokeshwar, Soum D. and Madhusoodanan, Vinayak and Ramasamy, Ranjith},
	month = sep,
	year = {2019},
	pages = {52},
}

@article{chen_method_2022,
	title = {A method for sperm activity analysis based on feature point detection network in deep learning},
	volume = {4},
	issn = {2624-9898},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2022.861495/full},
	doi = {10.3389/fcomp.2022.861495},
	abstract = {Sperm motility is an important index to evaluate semen quality. Computer-assisted sperm analysis (CASA) is based on the sperm image, through the image-processing algorithm to detect the position of the sperm target and track tracking, so as to judge the sperm activity. Because of the small and dense sperm targets in sperm images, traditional image-processing algorithms take a long time to detect sperm targets, while target-detection algorithms based on the deep learning have a lot of missed detection problems in the process of sperm target detection. In order to accurately and efficiently analyze sperm activity in the sperm image sequence, this article proposes a sperm activity analysis method based on the deep learning. First, the sperm position is detected through the deep learning feature point detection network based on the improved SuperPoint, then the multi-sperm target tracking is carried out through SORT and the sperm motion trajectory is drawn, and at last the sperm survival is judged through the sperm trajectory to realize the analysis of sperm activity. The experimental results show that this method can effectively analyze the sperm activity in the sperm image sequence. At the same time, the average detection speed of the sperm target detection method in the detection process is 65fps, and the detection accuracy is 92\%.},
	urldate = {2024-06-22},
	journal = {Frontiers in Computer Science},
	author = {Chen, Zhong and Yang, Jinkun and Luo, Chen and Zhang, Changheng},
	month = aug,
	year = {2022},
	pages = {861495},
}

@misc{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	doi = {10.48550/arXiv.2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jul,
	year = {2020},
	note = {arXiv:2002.05709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chang_gold-standard_2014,
	title = {Gold-standard and improved framework for sperm head segmentation},
	volume = {117},
	issn = {01692607},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260714002521},
	doi = {10.1016/j.cmpb.2014.06.018},
	language = {en},
	number = {2},
	urldate = {2022-11-23},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Chang, Violeta and Saavedra, Jose M. and Castañeda, Victor and Sarabia, Luis and Hitschfeld, Nancy and Härtel, Steffen},
	month = nov,
	year = {2014},
	pages = {225--237},
}

@misc{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	doi = {10.48550/arXiv.2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv:2104.14294},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chandra_prolificacy_2022,
	title = {Prolificacy {Assessment} of {Spermatozoan} via {State}-of-the-{Art} {Deep} {Learning} {Frameworks}},
	volume = {10},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9693937/},
	doi = {10.1109/ACCESS.2022.3146334},
	urldate = {2024-06-22},
	journal = {IEEE Access},
	author = {Chandra, Satish and Gourisaria, Mahendra Kumar and Gm, Harshvardhan and Konar, Debanjan and Gao, Xin and Wang, Tianyang and Xu, Min},
	year = {2022},
	pages = {13715--13727},
}

@inproceedings{carrillo_computer_2007,
	address = {New York},
	title = {A computer aided tool for the assessment of human sperm morphology},
	isbn = {978-1-4244-1509-0},
	url = {https://www.webofscience.com/wos/woscc/full-record/WOS:000252958200178},
	abstract = {Sperm morphology has a key role in the determination of fertility in men. Nowadays assessments of sperm morphology are mostly done with subjective criteria. This article discusses details of the stages implemented in a computer aided tool for the objective analysis of human sperm morphology, commonly known as Automated Sperm Morphology Analyzer (ASMA). The first stage described is the detection and extraction of individual spermatozoon of an image containing multiple spermatozoa and biological rests as well. A novel method for segmenting objects in microscopic images into its constituent's parts is proposed. The method called nth-fusion introduced in this paper is the framework of the segmentation algorithm implemented in this computer aided tool, this algorithm has been tested with a 250 spermatozoon images database and the results are quite accurate. The final stage, feature extraction and classification, estimates the morphological parameters of human spermatozoon and classifies them according to the World Healthcare Organization criteria.},
	language = {English},
	urldate = {2024-06-22},
	booktitle = {{PROCEEDINGS} {OF} {THE} {7TH} {IEEE} {INTERNATIONAL} {SYMPOSIUM} {ON} {BIOINFORMATICS} {AND} {BIOENGINEERING}, {VOLS} {I} {AND} {II}},
	publisher = {IEEE},
	author = {Carrillo, Henry and Villarreal, Jorge and Sotaquira, Miguel and Goelkel, Alvaro and Gutierrez, Ricardo},
	editor = {Yang, M. Q. and Zhu, M. M. and Zhang, Y. and Arabnia, H. R. and Deng, Y. and Bourbakis, N.},
	year = {2007},
	note = {Num Pages: 2
Web of Science ID: WOS:000252958200178},
	keywords = {ASMA, microscopic image, morphology, morphometry, segmentation, spermatozoon, spermiogram},
	pages = {1152--+},
}

@article{boumaza_computer-assisted_2020,
	title = {Computer-{Assisted} {Analysis} of {Human} {Semen} {Concentration} and {Motility}:},
	volume = {11},
	issn = {1947-315X, 1947-3168},
	shorttitle = {Computer-{Assisted} {Analysis} of {Human} {Semen} {Concentration} and {Motility}},
	url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJEHMC.2020100102},
	doi = {10.4018/IJEHMC.2020100102},
	abstract = {Computer-assisted semen analysis systems insist on evaluating sperm characteristics. These systems afford capacity to study and evaluate sperm statistical and morphological characteristics such as concentration, morphology, and motility, which have an important role in diagnosis and treatment of male infertility. In this paper, the proposed algorithm allows the assessment of concentration and motility rate of sperms in microscopic videos. First, enhancement process is required because of microscopic images limitations such as low contrast and noises. Then, for true sperm recognition among noise and debris, a hybrid approach is proposed using a combination between segmentation techniques. After, the use of geometric features of the bounding ellipse of the sperm head led to define sperm concentration. Finally, inter-frame difference is applied for motile sperm detection. The proposed method was tested on microscopic videos of human semen; the performance of this method is analyzed in terms of speed, accuracy, and complexity. Obtained results during the experiments are very promising compared with those obtained by the traditional assessment, which is the most widely used and approved in the laboratories.},
	language = {en},
	number = {4},
	urldate = {2022-11-23},
	journal = {International Journal of E-Health and Medical Communications},
	author = {Boumaza, Karima and Loukil, Abdelhamid},
	month = oct,
	year = {2020},
	pages = {17--33},
}

@misc{bochkovskiy_yolov4_2020,
	title = {{YOLOv4}: {Optimal} {Speed} and {Accuracy} of {Object} {Detection}},
	shorttitle = {{YOLOv4}},
	url = {http://arxiv.org/abs/2004.10934},
	doi = {10.48550/arXiv.2004.10934},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {\textasciitilde}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	month = apr,
	year = {2020},
	note = {arXiv:2004.10934},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{alhaj_alabdulla_robust_2021,
	title = {A robust sperm cell tracking algorithm using uneven lighting image fixing and improved branch and bound algorithm},
	volume = {15},
	issn = {1751-9659, 1751-9667},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/ipr2.12178},
	doi = {10.1049/ipr2.12178},
	language = {en},
	number = {9},
	urldate = {2022-11-23},
	journal = {IET Image Processing},
	author = {Alhaj Alabdulla, Ahmad and Haşıloğlu, Abdulsamet and Hicazi Aksu, Emrah},
	month = jul,
	year = {2021},
	pages = {2068--2079},
}

@article{bertolla_sperm_2020,
	title = {Sperm biology and male reproductive health},
	volume = {10},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-78861-7},
	doi = {10.1038/s41598-020-78861-7},
	language = {en},
	number = {1},
	urldate = {2024-06-22},
	journal = {Scientific Reports},
	author = {Bertolla, Ricardo P.},
	month = dec,
	year = {2020},
	pages = {21879, s41598--020--78861--7},
}

@article{basheera_classification_2019,
	title = {Classification of {Brain} {Tumors} {Using} {Deep} {Features} {Extracted} {Using} {CNN}},
	volume = {1172},
	issn = {1742-6596},
	url = {https://dx.doi.org/10.1088/1742-6596/1172/1/012016},
	doi = {10.1088/1742-6596/1172/1/012016},
	abstract = {Deep learning play a major role in medical automation, Convolutional Neural Networks (CNN) is an important machine learning technique for medical image segmentation and classification. In this work, we propose a novel approach that uses CNN\_S for classifying brain tumors in to normal and three different types. The tumor is initially segmented from the MRI images using an enhanced ICA mixture mode model. From the segmented image deep features are extracted and classified. The results have been validated by measuring the performance of classifier on a data set available with Harvard Medical School.},
	language = {en},
	number = {1},
	urldate = {2024-07-30},
	journal = {Journal of Physics: Conference Series},
	author = {Basheera, Shaik and Ram, M. Satya Sai},
	month = mar,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {012016},
}

@article{agatonovic-kustrin_basic_2000,
	title = {Basic concepts of artificial neural network ({ANN}) modeling and its application in pharmaceutical research},
	volume = {22},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {07317085},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0731708599002721},
	doi = {10.1016/S0731-7085(99)00272-1},
	language = {en},
	number = {5},
	urldate = {2024-06-22},
	journal = {Journal of Pharmaceutical and Biomedical Analysis},
	author = {Agatonovic-Kustrin, S and Beresford, R},
	month = jun,
	year = {2000},
	pages = {717--727},
}
